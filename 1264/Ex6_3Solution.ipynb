{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-On Exercise 6.3:\n",
    "# Working with Naïve Bayes in Python\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives\n",
    "\n",
    "#### In this exercise, you will work with a naive bayes model on unstructured data in Python. This exercise allows you to predict a target variable from a number of predictor variables. The goal is to show you how a naive bayes model can be used to predict unknown values from a model trained on an existing data set.\n",
    "\n",
    "### Overview\n",
    "\n",
    "You will work on a data set called sms_spam that you will import from a csv file. You will:<br>\n",
    "● Preprocess the unstructured data into a format suitable for naive bayes<br>\n",
    "● Examine the predictor variables<br>\n",
    "● Train a naive bayes model that can be used to make future predictions<br><br>\n",
    "\n",
    "1. ❏ Import the **CountVectorizer** library from **sklearn.feature_extraction.text**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. ❏ Import the **MultinomialNB** library from **sklearn.naive_bayes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ❏ Import the **pandas** library and use the **read_csv()** function to import the **sms_spam.csv** dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "sms = pd.read_csv('sms_spam.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. ❏ Count the unique values in the target variable **type**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     4827\n",
       "spam     747\n",
       "Name: type, dtype: int64"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sms['type'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. ❏ Examine the proportions in the target variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ham     86.598493\n",
       "spam    13.401507\n",
       "Name: type, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100 * sms['type'].value_counts() / len(sms['type'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. ❏ Separate the **text** variable into a dataframe called **Pred**, and the **type** variable into a dataframe called **target**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pred = sms['text']\n",
    "target = sms['type']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. ❏ Split the dataset into training and test datasets using the **train_test_split()** function<br><br>\n",
    "*Hint: train_test_split will need to be imported from the sklearn.model_selection library*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "Pred_train, Pred_test, target_train, target_test = train_test_split(Pred, target, test_size = 0.1, random_state = 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. ❏ Use the **CountVectorizer()** and **fit_transform()** functions to produce a Term Document Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "counts = vectorizer.fit_transform(Pred_train.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9. ❏ Assign the target values from the training dataset into a variable called **targets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "targets = target_train.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "10. ❏ Train the Naive Bayes model with the **MultinomialNB()** function using the Term Document Matrix and the **targets** variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classifier = MultinomialNB()\n",
    "\n",
    "classifier.fit(counts, targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "11. ❏ Vectorize the predictor data in the test dataset into token counts<br><br>\n",
    "*Hint: Use the initialized CountVectorizer() function from step 8*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_count = vectorizer.transform(Pred_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "12. ❏ Use the trained model to make predictions for the vectorized test dataset, and display them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam', 'ham',\n",
       "       'ham', 'spam', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'spam', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'spam',\n",
       "       'ham', 'spam', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'spam', 'spam', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'spam', 'spam', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham', 'ham',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham',\n",
       "       'spam', 'ham', 'ham', 'spam', 'ham', 'ham', 'ham', 'ham', 'spam',\n",
       "       'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'ham', 'spam', 'ham'],\n",
       "      dtype='<U4')"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classifier.predict(test_count)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "13. ❏ Evaluate the model using the **classification_report()** function<br><br>\n",
    "*Hint: This will need to be imported from sklearn.metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.98      0.99      0.99       469\n",
      "        spam       0.96      0.90      0.93        89\n",
      "\n",
      "    accuracy                           0.98       558\n",
      "   macro avg       0.97      0.95      0.96       558\n",
      "weighted avg       0.98      0.98      0.98       558\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(target_test, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "14. ❏ Evaluate the model using the **confusion_matrix()** function<br><br>\n",
    "*Hint: This will need to be imported from sklearn.metrics*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[466,   3],\n",
       "       [  9,  80]], dtype=int64)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(target_test,predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>**Congratulations! You have completed the exercise.**</center>"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAC8AAAAgCAYAAACCcSF5AAAFlElEQVRYCb2YW0hUXRTHdWJKMtIsyuyCqVFQRIVhN0UriKgwKnqSekgoIqOIHkpfii5vUmQ9dYEeg3wKwgiCQf38TGecGa9pauOkjZmN19SZ8f/xX7XPN5nHzozmgTP7nLX3Xvu31l577X0mAtO4/L29aD58GJUREagyeL9jO7MZ7vx8jAcC0xgdiAi390hzM2q3bsW/BqEnGlcREYGWEycwPjoaLkJ48ENVVXCsWRM2uDKEBjQfPYrxkZGwDAjZ8/1v3qBm6dJpg/9iQHY2AsPDIRsQEnzv8+eojo6eMfBfDDh0CH6vNyQDDMN3P3yIdyZTSItTwRkpGUJNmZnwf/1q2ABD8J23bom3JVOEuUCNGtCYkQFfT48hA/4I77p4EfTK3wZXxnGsxvR0+Lq7/2iALry/vx9tJ0/OKrgygOm3YedOjHV1TWmALnzPo0con0WPK3BVyhrYswfjPp+uAbrwnnv3NK/TE1TGO3hT4s6q5BPL4F1Xrz9Bg+uUDsrY3xYXh8DgYOjw3Q8eiOc5wIdjx+C6cAGu8+fRuGuXKKbyug0bfsjz8vDx7Fl8PHMGrrw8kdWnpgoYQRjD7Esd7w8c0PpzHTVlZaGd/X7Wfzx3Dg3bt4tT7MuXIzA0FDq85+5dVJrN+FZc/Ftnd0EByk0mtJ8+/VudEnRcuYIKkwlfnj5VIq30lpTAGhsrgJPpH/f70XL8OGwLFoQH33nzpuRdjtjz5MkPT8THY7CyEsN2O2yxsaiaOxfWmBjYExIw0tqKMY8HzsREkdHwT9euCXDf69ewJyejOiYGXbdvi+zbixcyMz3Pnsl7Y2YmKmNi0JaTI+8DpaWwr1gBJg69Szfmu+7cQf3mzdJvzO3Gp+vX4UxJ0aa8KjJS0ifDp9psBg9qzA70FmNXZB8+yLnFmZQkMrblPex0il464Mvjx/LsSEqChVkmNVXe2caRmAh/X5+8T/ajC++5f19i3n31KvwDA1rfAYsFjVlZAsH1wLi1RkdjpKUFY58/oyYuTjzK2WC8jrx/D2Uo2xPe++qV6GtIS4OnqEieh6xW9Fss2gLtuHwZtoULtXcNIOhBF54L9p+f2cS+ciVasrPRW1wsyniMrdu0SUD04GsWLxav+To7YY2K0owlPEOCV+26deCxg9eQzSbwfSUlaM/NFQfULFsWXswzNttyczFcW4vG3btlSjmtnwoKZDBmF2aSyeAJSDljnRdhuGfwZiYZHxuDz+NB1Zw52oJ2rl8v9dSpbsZ8WNlGYj4tTQbnWaMjPx9cxIH+fhm8bsuWXzw/2tEBn9cLepzgkiIzMrSj7ufCQnTeuIHAzxBszcmRmeVJlVf9tm3Sh6GlwkuFnjSY5Ec/bIqKUMaPhYMHZYExVPjRwClv2r9fCwPx/Pz5GCgrk6mviY0VeALQgIb0dHhfvkTg+3fpP1hRIR8grOMMMeaZEBiGlM0YPLOGKDSZJG4ZuwpKDaLK6nnzUM36yEgNQLUVA6OiRAdl1Kv1M5t/yE0mTcY6Gha25z2FhTKtVMTBg281cHCp6oNl6lnVqVLJWU4mo5zw/GIL63jAzYi7YPBUBg/6t585O45VqxCY4vtWN+a5PgbevoV10aJZN4AO47hMm1NdU8KLARYLbEuWaAt0Njxek5CAwfLyqbil7o/wbDVYWgrbDP5joOcAepxnIJ6djFyG4MWAsjLY4uP/WggR3Llxo5yRjICzjWF4Nub5wz4DfzZN9DwXZ/2OHRh1u41yS7uQ4Nlj2OEAT4AzlYWop2nfPvhC+MtDWRgyPDt+r62FIzl5WgYwv9PjLUeOaEcGBWW0DAteDGhogHPtWgFQG83EcmJ4BL8TvPXUqdn/o1V5ZtTlQtPevXCsXg1ncvL/d0oK+AHCI0MwsHrm7um6dEmpCbv8D7qC8IEdzCaUAAAAAElFTkSuQmCC"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>**This is the end of the exercise.**</center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
